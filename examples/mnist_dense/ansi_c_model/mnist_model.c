/*
    Code generated by TFMin tensorflow to ansi-c converter.
    Do not edit.
*/
#include "mnist_model.h"

void mnist_model(void *tensor_arena, float *const p_input_47x_45input_580,
                 float *p_Layer2_47activation_580) {
  /* MatMul op (Layer1/Wx_plus_b/MatMul) */
  {
    const float *input_0 = p_input_47x_45input_580;
    const float *input_1 =
        (float *)mnist_Layer1_47weights_47Variable_47read_580;
    const float *input_2 = (float *)mnist_Layer1_47biases_47Variable_47read_580;
    float *output_0 = (float *)tensor_arena + 0;

    // 784 is the last dimension of the input tensor
    // 300 is the last dimension of the output tensor
    const float *weightsData = input_1;
    for (int b = 0; b < 1; ++b) {
      for (int out_c = 0; out_c < 300; ++out_c) {
        float value = 0.0f;
        for (int d = 0; d < 784; ++d) {
          float inputVal = input_0[(b * 784) + d];
          // float weightVal = weightsData[(out_c * 784) + d];
          // TODO need to work out why the weights are transposed so this
          // line has needed altering.
          float weightVal = weightsData[(out_c + (300 * d))];
          value += inputVal * weightVal;
        }
        // Add Bias
        value += input_2[out_c];
        // fused activation function
        // Relu
        if (value < 0) value = 0;
        output_0[out_c + (300 * b)] = value;
      }
    }
  }
  /* MatMul op (Layer2/Wx_plus_b/MatMul) */
  {
    const float *input_0 = (float *)tensor_arena + 0;
    const float *input_1 =
        (float *)mnist_Layer2_47weights_47Variable_47read_580;
    const float *input_2 = (float *)mnist_Layer2_47biases_47Variable_47read_580;
    float *output_0 = p_Layer2_47activation_580;

    // 300 is the last dimension of the input tensor
    // 10 is the last dimension of the output tensor
    const float *weightsData = input_1;
    for (int b = 0; b < 1; ++b) {
      for (int out_c = 0; out_c < 10; ++out_c) {
        float value = 0.0f;
        for (int d = 0; d < 300; ++d) {
          float inputVal = input_0[(b * 300) + d];
          // float weightVal = weightsData[(out_c * 300) + d];
          // TODO need to work out why the weights are transposed so this
          // line has needed altering.
          float weightVal = weightsData[(out_c + (10 * d))];
          value += inputVal * weightVal;
        }
        // Add Bias
        value += input_2[out_c];
        // fused activation function
        // None
        output_0[out_c + (10 * b)] = value;
      }
    }
  }
}
