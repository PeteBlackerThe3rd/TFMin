/*
    TFMin v1.0 Minimal TensorFlow to C++ exporter
    ------------------------------------------

    Copyright (C) 2019 Pete Blacker, Surrey Space Centre & Airbus Defence and Space Ltd.
    Pete.Blacker@Surrey.ac.uk
    https://www.surrey.ac.uk/surrey-space-centre/research-groups/on-board-data-handling

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    in the LICENCE file of this software.  If not, see
    <http://www.gnu.org/licenses/>.

    ---------------------------------------------------------------------

    Simple example integrating the c++ MNIST classification inference model
    generated by TFMin with an application
*/
#include <iostream>
#include "mnist_model.h"
#include "example_mnist_input.h"

int main()
{
    // Instantiate mnist inference model object
    MNISTModel mnist;
    
    // Create single threaded executionEigen device
    Eigen::DefaultDevice device;

    // setup input and output buffers
    float *input = (float*)exampleInputDataHex;
    float output[10];
    
    /*std::cout << "Running inference model." << std::endl;
    mnist.eval(device, input, output);
    
    std::cout << "Completed output was." << std::endl;
    for (int i=0; i<10; ++i)
        std::cout << "[" << i << "] = " << output[i] << std::endl;*/

        std::cout << "about to verify model." << std::endl;
	if (mnist.validate(device))
		std::cout << "Verification Passed." << std::endl;
	else
	{
		std::cout << "Error: Verification Failed." << std::endl;
		return 1;
    }

    std::cout << "get time performance of model." << std::endl;
    TFMin::TimingResult times = mnist.timing(device, input, output, false);
    mnist.printTiming(times);
	std::cout << "Completed running model." << std::endl;

    return 0;
}