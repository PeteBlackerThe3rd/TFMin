"""
    TFMin v1.0 Minimal TensorFlow to C++ exporter
    ------------------------------------------

    Copyright (C) 2019 Pete Blacker, Surrey Space Centre & Airbus Defence and
    Space Ltd.
    Pete.Blacker@Surrey.ac.uk
    https://www.surrey.ac.uk/surrey-space-centre/research-groups/on-board-data-handling

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    in the LICENCE file of this software.  If not, see
    <http://www.gnu.org/licenses/>.

    ---------------------------------------------------------------------

    This module contains core ansi-c code generator object. This uses the
    set of operation kernels to generate the c implementation of each layer
    of the model in the provided graph.
"""
import scipy.io
import numpy as np
import struct as st
import argparse
import sys
import os

from tf_min import graph as tfm_g
from tf_min import types
from tf_min import cpp_code_gen as c_gen

from tf_min.v2_kernels import *
from tf_min.v2_kernels.base_op_kernel import BaseOpKernel
from tf_min.v2_kernels.pooling import PoolingOpKernel


class CodeGenerator:
    """
    TFMin ansi-c code generator object.
    """
    BOILER_PLATE = "/* \n    Code generated by TFMin tensorflow to ansi-c " \
                   "converter.\n    Do not edit.\n*/\n"

    def __init__(self, graph, base_name="model_", prefix="", path=".",
                 clang_format=None, byte_order="@", batch_size=1):
        """

        """
        # verify that this graph has been sequenced and intermediate
        # tensors have been pre-allocated.
        self.tensor_arena_size = graph.get_peak_memory()
        if self.tensor_arena_size is not None and graph.sequenced():
            # save model and export params
            self.graph = graph
            self.base_name = base_name
            self.prefix = prefix
            self.path = path
            self.clang_format = clang_format
            self.byte_order = byte_order
            self.batch_size = batch_size
            self.export_ready = True
        else:
            # model is not ready for export,
            print("Error: graph given to CodeGeneration object is not ready"
                  "for export.")
            self.graph = None
            self.base_name = None
            self.prefix = None
            self.path = None
            self.clang_format = None
            self.byte_order = None
            self.batch_size = 1
            self.export_ready = False

    def __call__(self, silent=False):
        """

        :return: True if expport successful, False operation
        """
        if not self.export_ready:
            print("Error: Cannot generate code for a graph which isn't ready")
            return

        # ensure the output path exists
        if not os.path.exists(self.path):
          os.mkdir(self.path)

        # Generate the four source files containing this model
        if not silent:
            print("Generating weight header.")
        wh_okay = self.gen_weights_header()
        if not silent:
            if wh_okay:
                print("Complete.")
            else:
                print("Failed.")

        if not silent:
            print("Generating weight source.")
        ws_okay = self.gen_weights_source()
        if not silent:
            if ws_okay:
                print("Complete.")
            else:
                print("Failed.")

        if not silent:
            print("Generating model header.")
        mh_okay = self.gen_model_header()
        if not silent:
            if mh_okay:
                print("Complete.")
            else:
                print("Failed.")

        if not silent:
            print("Generating model source.")
        ms_okay = self.gen_model_source()
        if not silent:
            if ms_okay:
                print("Complete.")
            else:
                print("Failed.")

        if self.clang_format is not None:
            filenames = [os.path.join(self.path, self.base_name + 'model.c'),
                         os.path.join(self.path, self.base_name + 'model.h'),
                         os.path.join(self.path, self.base_name + 'weights.c'),
                         os.path.join(self.path, self.base_name + 'weights.h')]
            clang_command = ("clang-format %s -i -style=%s" % (
              " ".join(filenames),
              self.clang_format
            ))
            clang_stream = os.popen(clang_command)
            clang_result = clang_stream.read().strip()
            if clang_result == "":
                print("Formatted code to style %s okay." % self.clang_format)
            else:
                print("Failed to format code with error :\n%s" % clang_result)

        if wh_okay and ws_okay and mh_okay and ms_okay:
            if not silent:
                print("All source files generated okay.")
            return True
        else:
            return False

    def gen_weights_header(self):
        """
        Method to generate the header file for the model weights, also
        includes a macro defining the tensor areana size
        :Returns: True if successful, False otherwise
        """
        file_name = self.base_name + "weights.h"
        try:
            file = open(os.path.join(self.path, file_name), "w")
            file.write(CodeGenerator.BOILER_PLATE)
            file.write(self.gen_opening_include_guard(file_name))
            file.write("#include <inttypes.h>\n\n")

            # Add tensor arena size macro
            file.write("#define %sTENSOR_ARENA_SIZE %d /* size of the tensor "
                       "area in bytes */\n\n" % (self.prefix.upper(),
                                                 self.tensor_arena_size))

            # Add tesnor weight prototypes
            for weight in self.graph.get_constants():
                file.write(self.gen_weight_proto(weight))
            file.write("\n")

            file.write(self.gen_closing_include_guard(file_name))
            return True
        except IOError as e:
            print(e)
            return False

    def gen_weights_source(self):
        """Generate"""
        file_name = self.base_name + "weights.c"
        try:
            file = open(os.path.join(self.path, file_name), "w")
            file.write(CodeGenerator.BOILER_PLATE)
            file.write("#include <%sweights.h>\n\n" % self.base_name)

            # Add tesnor weight definitions
            for weight in self.graph.get_constants():
                file.write(self.gen_weight_def(weight))

            return True
        except IOError as e:
            print(e)
            return False

    def gen_model_header(self):
        """Generate"""

        file_name = self.base_name + "model.h"
        try:
            file = open(os.path.join(self.path, file_name), "w")
            file.write(CodeGenerator.BOILER_PLATE)
            file.write(self.gen_opening_include_guard(file_name))
            file.write("#include <%sweights.h>\n\n" % self.base_name)

            # define model function prototype
            file.write("void %smodel(%s);\n\n" % (
                self.prefix,
                self.gen_function_paramers()
            ))

            file.write(self.gen_closing_include_guard(file_name))
            return True
        except IOError as e:
            print(e)
            return False

    @staticmethod
    def find_kernel(operation, tags):
        """

        :param operation:
        :param tags:
        :return:
        """
        for kernel in BaseOpKernel.__subclasses__():
            if kernel.matches(operation):
              return kernel

        return None

    def gen_model_source(self):
        """Generate"""
        file_name = self.base_name + "model.c"
        try:
            file = open(os.path.join(self.path, file_name), "w")
            file.write(CodeGenerator.BOILER_PLATE)
            # file.write("#include <float.h>\n")
            file.write("#include <%smodel.h>\n" % self.base_name)

            # find the operation kernel for each layer and find all
            # dependencies
            dependencies = {}
            for operation in self.graph.op_sequence:
                kernel = CodeGenerator.find_kernel(operation, tags=[])
                if kernel is not None:
                    kernel_instance = kernel(operation)
                    dependencies.update(kernel_instance.get_dependencies())
            for dependency in dependencies.keys():
                file.write("#include <%s>\n" % dependency)
            file.write("\n")

            # declare model function body
            file.write("void %smodel(%s) {\n" % (
                self.prefix,
                self.gen_function_paramers()
            ))

            # write a code block for each operation
            for operation in self.graph.op_sequence:
                file.write("    /* %s op (%s) */\n" % (operation.type,
                                                       operation.label))
                file.write("    {\n")

                kernel = CodeGenerator.find_kernel(operation, tags=[])
                if kernel is not None:
                    kernel_instance = kernel(operation)
                    file.write(kernel_instance.generate(self.batch_size,
                                                        self.prefix))
                else:
                    print("Error: Couldn't find kernel to generate "
                          "code for %s operation." % operation.type)

                file.write("    }\n")

            file.write("}\n")

            return True
        except IOError as e:
            print(e)
            return False

    def gen_file_name(self, suffix):
        """
        Method to generate a full filename from path, base and suffix
        :param suffix: String, the specific source file suffix.
        :return: String, the full filename for this source file.
        """
        return os.path.join(self.path, self.base_name) + suffix

    @staticmethod
    def gen_opening_include_guard(file_name):
        """

        :param file_name:
        :return:
        """
        macro = "__" + file_name.replace(".", "_").upper() + "__"
        return "#ifndef %s\n#define %s\n" % (macro, macro)

    @staticmethod
    def gen_closing_include_guard(file_name):
        """

        :param file_name:
        :return:
        """
        macro = "__" + file_name.replace(".", "_").upper() + "__"
        return "#endif /* %s */\n" % macro

    def gen_weight_proto(self, tensor):
        """

        :param tensor: TFMin.Tensor object
        :return: String, the prototype of the weights const global definition
        """
        identifier = self.prefix + c_gen.c_safe_identifier(tensor.label)
        return "extern const uint32_t %s[];\n" % identifier

    def gen_function_paramers(self):
        """
        Method to generate a string containing the parameter definitions for
        a model function. All parameters are pointers.
        :return: String
        """
        identifiers = ['tensor_arena']
        d_types = ['void']
        for input in self.graph.get_inputs():
            identifiers.append('const p_' +
                               c_gen.c_safe_identifier(input.label))
            d_types.append(types.get_dtype_c_type(input.d_type))
        for output in self.graph.get_outputs():
            identifiers.append('p_' + c_gen.c_safe_identifier(output.label))
            d_types.append(types.get_dtype_c_type(output.d_type))
        params = []
        for idx, identifier in enumerate(identifiers):
            params.append(d_types[idx] +
                          " *" + identifier)
        return ", ".join(params)

    def gen_weight_def(self, tensor):
        """

        :param tensor:
        :return:
        """
        identifier = self.prefix + c_gen.c_safe_identifier(tensor.label)

        if tensor.value is None:
          print("Tensor [%s] value is None!" % tensor.label)

        if not isinstance(tensor.value, np.ndarray):
            tensor.value = np.array(tensor.value)

        # the value of constant tensors is stored in a np.ndarray the layout
        # will appropriate for direct conversion to a c array layout
        value_flat = tensor.value.flatten(order='C')
        # print("value_flat is [%s]" % str(value_flat))
        # print("flat size is [%s]" % value_flat.size)

        # Constant literals are always written as arrays of unsigned 32 bit
        # integers in hex. This is both a dense format and a precise way of
        # representing floating point values in text.
        struct_type = types.get_dtype_struct_type(tensor.d_type)
        value_buffer = st.pack(self.byte_order + struct_type * value_flat.size,
                               *value_flat)
        # print("Buffer len [%d]" % len(value_buffer))

        # pad the buffer to a multiple of 4 bytes
        pad_len = 4 - len(value_buffer) % 4
        if pad_len == 4:
            pad_len = 0
        value_buffer += bytearray([0] * pad_len)
        # print("Buffer len padd [%d]" % len(value_buffer))

        # write definition as one line (clang format will clean it up)
        int_values = st.unpack(self.byte_order + "I" * int(len(value_buffer)/4),
                               value_buffer)
        hex_values = map(hex, int_values)
        return "const uint32_t %s[] = { %s };\n" % (identifier,
                                                    ", ".join(hex_values))
